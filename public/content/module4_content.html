<h2>Slide 1: Why Ethics and Governance Matter</h2>
<p>AI in healthcare isn’t just about accuracy. It’s about safety, fairness, transparency, and accountability. An algorithm might work brilliantly in a lab—but if it amplifies bias, fails in new populations, or can’t be explained, it becomes a liability.</p>
<p>The WHO (2021) and FDA (2021) both stress that governance is central to trustworthy AI. Let’s unpack why that matters for clinicians.</p>

<h2>Slide 2: The WHO’s Six Principles for AI in Health</h2>
<p>The World Health Organization outlines six principles for ethical AI:</p>
<ul>
    <li>Protect human autonomy</li>
    <li>Promote human well-being and safety</li>
    <li>Ensure transparency, explainability, and intelligibility</li>
    <li>Foster responsibility and accountability</li>
    <li>Ensure inclusiveness and equity</li>
    <li>Promote sustainable AI</li>
</ul>
<blockquote>"AI should augment clinical judgement, not replace it. And it should serve all populations, not just the data-rich." – WHO, 2021</blockquote>
<p><em>Clinical relevance:</em> These principles guide procurement, validation, and deployment decisions—not just ethics committees.</p>

<h2>Slide 3: AI Bias Is a Clinical Risk</h2>
<p>AI bias isn’t just theoretical. It’s a patient safety issue.</p>
<p>Obermeyer et al. (2019, Science) showed that a US risk algorithm underestimated health needs of Black patients due to biased training data.</p>
<blockquote>"Bias in healthcare algorithms can perpetuate or amplify existing disparities in access and outcomes." – Obermeyer et al., 2019</blockquote>
<p>How it happens:</p>
<ul>
    <li>Training data lacks representation</li>
    <li>Labels reflect unequal care</li>
    <li>Shortcut features dominate learning (e.g., insurance status)</li>
</ul>
<p>Result: AI offers less care or lower triage priority to marginalised groups.</p>

<p>References:<br>
<em>  1. Obermeyer Z, Powers B, Vogeli C, Mullainathan S. Dissecting racial bias in an algorithm used to manage the health of populations. Science. 2019;366(6464):447-453. doi:10.1126/science.aax2342</em></p>

<h2>Slide 4: Explainability: Can the Model Be Interpreted?</h2>
<p>Clinicians need to understand why an AI gave a certain output.</p>
<p>Lipton (2018) argues that interpretability is essential when decisions affect life and liberty.</p>
<p>There are two types:</p>
<ul>
    <li>Transparent models (e.g., decision trees, logistic regression)</li>
    <li>Post hoc explainability (e.g., saliency maps for CNNs)</li>
</ul>
<p><em>Limitation:</em> Not all explanations are reliable. Saliency maps can be misleading.</p>
<p><em>Clinical rule of thumb:</em> If you wouldn’t base treatment on a black box, don’t use one without clear justification.</p>
<p>References:<br>
<em>  1. 1. Lipton ZC. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue. 2018;16(3):31-57. doi:10.1145/3236386.3241340</em></p>


<h2>Slide 5: Regulatory Bodies Are Catching Up</h2>
<p>The FDA’s 2021 action plan on AI/ML-based Software as a Medical Device (SaMD) introduced guidelines for:</p>
<ul>
    <li>Transparency and documentation</li>
    <li>Continuous learning systems</li>
    <li>Real-world performance monitoring</li>
</ul>
<blockquote>"AI tools must be safe and effective across their lifecycle." – FDA, 2021</blockquote>
<p>The EU AI Act and GDPR also impose strict rules on algorithmic fairness, data access, and the "right to explanation."</p>
<p>Clinicians must ensure tools they use are compliant with national and institutional policies.</p>

<h2>Slide 6: Who Is Responsible When AI Goes Wrong?</h2>
<p>Accountability is often unclear. If an AI misses a diagnosis or causes harm:</p>
<ul>
    <li>Is the developer liable?</li>
    <li>Is the clinician responsible for using it?</li>
    <li>Is the institution accountable for procurement?</li>
</ul>
<p>Legal frameworks are still evolving. Until then:</p>
<p>You are responsible for your clinical decisions—even if guided by AI.</p>
<blockquote>"Human oversight remains essential in AI-assisted healthcare decisions." – WHO, 2021</blockquote>

<h2>Slide 7: How to Evaluate Ethical Readiness</h2>
<p>Before using any AI tool, clinicians should ask:</p>
<ul>
    <li>Was the data diverse and inclusive?</li>
    <li>Were harms and limitations clearly reported?</li>
    <li>Is the output explainable?</li>
    <li>Has it undergone real-world testing?</li>
    <li>Is there a feedback mechanism for errors?</li>
</ul>
<p>If the answer to most of these is 'no', the tool may not be ethically ready for clinical use.</p>

<h2>Slide 8: Post-deployment Surveillance and Audit</h2>
<p>Deployment isn’t the end. It’s the beginning of oversight.</p>
<blockquote>"AI tools can drift in performance as clinical practice and data evolve." – FDA, 2021</blockquote>
<p>What this means:</p>
<ul>
    <li>Continuous monitoring is necessary (e.g., changes in AUROC, calibration)</li>
    <li>Flagging errors and near misses must be built in</li>
    <li>Clinicians should be trained to recognise and report AI failures</li>
</ul>
<p>Clinical systems should treat AI tools like junior staff: trust, but verify.</p>

<h2>Slide 9: Summary Takeaways</h2>
<ul>
    <li>AI that works in theory may fail in practice without proper governance.</li>
    <li>Ethical AI requires fair data, meaningful oversight, and real-world validation.</li>
    <li>Explainability isn’t optional when patient safety is at stake.</li>
    <li>Regulatory standards are evolving—clinicians must stay informed.</li>
    <li>If you’re using AI, you’re part of the accountability chain.</li>
</ul>
<p>Your role: Be the human in the loop. Ask what the AI knows, and what it might miss.</p>

<!-- Quiz Placeholder - The quiz content is handled by the Quiz component -->
<div id="quiz-placeholder"></div>
