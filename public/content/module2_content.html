<h2>Slide 1 â€“ Letâ€™s Recap</h2>
<p>From Module 1, you learned:</p>
<ul>
    <li>AI learns from examples, not rules</li>
    <li>It builds patternsâ€”not understanding</li>
    <li>It performs well only if the training data was fair and relevant</li>
</ul>
<p>Now, weâ€™ll explore how it learns, and what can go wrong inside that black box.</p>

<h2>Slide 2 â€“ A Simple Learning Analogy</h2>
<p>Imagine teaching a child to tell cats from dogs.<br/>You donâ€™t give rules. You show 500 pictures:<br/>â€œThis is a cat.â€ â€œThis is a dog.â€</p>
<p>The child starts picking up featuresâ€”ears, tails, size.<br/>But if all the â€œdogsâ€ are outdoors and all the â€œcatsâ€ are indoors, they might just learn:<br/>â€œOutside = dog.â€</p>
<p>Thatâ€™s how AI can learn the wrong thing.</p>

<h2>Slide 3 â€“ So, What Is a Model?</h2>
<p>A model is the AIâ€™s memory of what patterns go with what outcomes.</p>
<p>It doesnâ€™t remember individual cases.<br/>It builds statistical shortcuts.</p>
<p>âœ… A model can detect subtle signals faster than any human<br/>âŒ But it will also learn shortcutsâ€”even bad onesâ€”if theyâ€™re present in the data</p>

<h2>Slide 4 â€“ The Goal Isnâ€™t Perfection on Training Data</h2>
<p>Good models donâ€™t just memoriseâ€”they generalise.</p>
<p>They work on new patients just as well as on old ones.</p>
<p>Thatâ€™s why we donâ€™t only ask: â€œHow accurate is it?â€<br/>We ask: â€œDoes it still work on data itâ€™s never seen before?â€</p>

<h2>Slide 5 â€“ What Is Overfitting?</h2>
<p>Overfitting = the model memorised the training set instead of learning general patterns.</p>
<p>ğŸ§  It performs beautifully on old data<br/>ğŸ˜“ And fails badly on new data</p>
<p>Common signs:</p>
<ul>
    <li>Too-perfect performance on training cases</li>
    <li>Big drop in external validation</li>
    <li>Learning irrelevant details (e.g., scanner brand)</li>
</ul>

<h2>Slide 6 â€“ Two Real Examples</h2>
<p>ğŸ§ª A model â€œlearnedâ€ that images with portable X-rays = pneumonia<br/>â†’ because more pneumonia patients were scanned that way.</p>
<p>ğŸ§ª A tool trained on light-skinned patients underperformed on dark skin<br/>â†’ because it didnâ€™t learn enough variation in the training set.</p>
<p>These arenâ€™t bugsâ€”theyâ€™re features of the data. The model was doing exactly what it was told.</p>

<h2>Slide 7 â€“ What Makes a Model Trustworthy?</h2>
<p>âœ… Diverse training data (different hospitals, populations)<br/>âœ… External validation<br/>âœ… Labels confirmed by gold standards (e.g., biopsy, consensus)<br/>âœ… No obvious shortcuts or confounders</p>
<p>If it wasnâ€™t tested in a real-world setting, be cautious.</p>

<h2>Slide 8 â€“ Questions to Ask Before You Trust a Model</h2>
<p>Was it trained on patients like mine?</p>
<p>Was it tested outside the original institution?</p>
<p>Are the predictions clinically sensible?</p>
<p>Were the labels accurate and consistent?</p>
<p>If you canâ€™t answer these, donâ€™t trust the AI blindly.</p>

<h2>Slide 9 â€“ Summary (Key Messages)</h2>
<ul>
    <li>AI learns statistical shortcuts, not meanings</li>
    <li>Itâ€™s only as good as its training data</li>
    <li>Overfitting happens when the AI learns things that donâ€™t generalise</li>
    <li>Clinicians must look at validation, population, and data quality</li>
</ul>

<!-- Quiz Placeholder -->
<div id="quiz-placeholder"></div>

